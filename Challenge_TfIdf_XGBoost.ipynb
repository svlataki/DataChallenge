{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge_TfIdf_XGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svlataki/DataChallenge/blob/main/Challenge_TfIdf_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Rm22qN8dkO0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cc63f7-ad2c-47f2-ec33-a97bffef8d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: greek-stemmer-pos in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: pytest-cov in /usr/local/lib/python3.7/dist-packages (from greek-stemmer-pos) (2.9.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from greek-stemmer-pos) (3.6.4)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->greek-stemmer-pos) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->greek-stemmer-pos) (8.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->greek-stemmer-pos) (57.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->greek-stemmer-pos) (21.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->greek-stemmer-pos) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->greek-stemmer-pos) (1.15.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->greek-stemmer-pos) (1.4.0)\n",
            "Requirement already satisfied: coverage>=4.4 in /usr/local/lib/python3.7/dist-packages (from pytest-cov->greek-stemmer-pos) (6.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "!pip install greek-stemmer-pos\n",
        "!pip install nltk\n",
        "import nltk\n",
        "import pickle\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from greek_stemmer import stemmer as greek_stemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Data_Challenge"
      ],
      "metadata": {
        "id": "B5CZ946DkZFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d1a930-a6af-430a-cb19-c3de55f3df8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Data_Challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read training data\n",
        "train_domains = list()\n",
        "y_train = list()\n",
        "with open(\"train.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        l = line.split(',')\n",
        "        train_domains.append(l[0])\n",
        "        y_train.append(l[1][:-1])\n",
        "\n",
        "# Read test data\n",
        "test_domains = list()\n",
        "with open(\"test.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        l = line.split(',')\n",
        "        test_domains.append(l[0])"
      ],
      "metadata": {
        "id": "vz3aHOa7kZck"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read textual content of webpages of domain names\n",
        "text = dict()\n",
        "with zipfile.ZipFile('domains.zip', \"r\") as zfile:\n",
        "    for filename in zfile.namelist():\n",
        "        if re.search(r'\\.zip$', filename) is not None:\n",
        "            zfiledata = BytesIO(zfile.read(filename))\n",
        "            with zipfile.ZipFile(zfiledata) as zfile2:\n",
        "                text[filename[:-4]] = ''\n",
        "                for name2 in zfile2.namelist():\n",
        "                    file = zfile2.read(name2)\n",
        "                    text[filename[:-4]] += file.decode('utf16') + ' '"
      ],
      "metadata": {
        "id": "hIELItAnkbOr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve textual content of domain names of the training set\n",
        "train_data = list()\n",
        "for domain in train_domains:\n",
        "    if domain in text:\n",
        "        train_data.append(text[domain])\n",
        "    else:\n",
        "        train_data.append('')"
      ],
      "metadata": {
        "id": "uTc2bCPUke7w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve textual content of domain names of the test set\n",
        "test_data = list()\n",
        "for domain in test_domains:\n",
        "    if domain in text:\n",
        "        test_data.append(text[domain])\n",
        "    else:\n",
        "        test_data.append('')\n",
        "\n",
        "# To reduce memory \n",
        "text = None"
      ],
      "metadata": {
        "id": "EeMa90FJkhDs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess data\n",
        "def data_preprocessing(data):\n",
        "    stemmer = WordNetLemmatizer()\n",
        "    docs = []\n",
        "\n",
        "    for doc in data:\n",
        "        #remove special characthers\n",
        "\n",
        "        document = re.sub(\"\\\\<.*?\\\\>\", \"\",doc);        \n",
        "\n",
        "        # Remove non-word characters such as numbers etc\n",
        "\n",
        "        document = re.sub(r'\\W', ' ', document)\n",
        "\n",
        "        # Remove all single characters\n",
        "\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)    \n",
        "\n",
        "        # Remove single characters from the start\n",
        "\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)        \n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)    \n",
        "\n",
        "        # remove digit characters\n",
        "\n",
        "        document = re.sub(r'[0-9]', '', document)\n",
        "\n",
        "        # Split the document in whitespaces (--> List of words)\n",
        "\n",
        "        document = document.split()  \n",
        "\n",
        "        # Lemmatize each word in the list\n",
        "\n",
        "        document = [stemmer.lemmatize(word) for word in document]\n",
        "\n",
        "        # stem each greek word in the list\n",
        "        document = [greek_stemmer.stem_word(word,'NNM') for word in document]\n",
        "\n",
        "        document = [w for w in document if len(w) > 3]\n",
        "\n",
        "        # Reconstruct the document by joining the words using whitespace\n",
        "\n",
        "        document = ' '.join(document)\n",
        "\n",
        "        # Gather all the documents\n",
        "\n",
        "        docs.append(document)\n",
        "\n",
        "\n",
        "\n",
        "    return docs"
      ],
      "metadata": {
        "id": "FzbsLOWgZ2s6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_preprocessed = data_preprocessing(train_data)\n",
        "pickle.dump(train_data_preprocessed,open(\"train_data_preprocessed.pkl\" ,'wb'))\n",
        "test_data_preprocessed = data_preprocessing(test_data)\n",
        "pickle.dump(test_data_preprocessed,open(\"test_data_preprocessed.pkl\" ,'wb'))"
      ],
      "metadata": {
        "id": "VMBtn37PssFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_data_preprocessed.pkl', 'rb') as pickle_file:\n",
        "    train_data_preprocessed = pickle.load(pickle_file)\n",
        "with open('test_data_preprocessed.pkl', 'rb') as pickle_file:\n",
        "    test_data_preprocessed = pickle.load(pickle_file)\n"
      ],
      "metadata": {
        "id": "babhKbqfOb95"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_dev, Y_train, Y_dev = train_test_split(train_data_preprocessed, y_train, test_size=0.2,stratify=y_train, random_state=42)"
      ],
      "metadata": {
        "id": "BRVBJrpwCdQW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training matrix. Each row corresponds to a domain name and each column to a word present in at least 10 webpages \n",
        "# and at most 50 webpages of domain names. The value of each entry in a row is equal to the tf-idf weight of that word in the \n",
        "# corresponding domain       \n",
        "vec = TfidfVectorizer(decode_error='ignore', strip_accents='unicode', min_df=10, max_df=50, ngram_range=(1,3), stop_words=stopwords.words('english'))\n",
        "X_train_transformed = vec.fit_transform(X_train)\n",
        "\n",
        "# Create the test matrix following the same approach as in the case of the training matrix\n",
        "X_test_transformed = vec.transform(test_data_preprocessed)\n",
        "\n",
        "#print(\"Train matrix dimensionality: \", train_data_preprocessed.shape)\n",
        "#print(\"Test matrix dimensionality: \", test_data_preprocessed.shape)"
      ],
      "metadata": {
        "id": "0T3xUmY2kiF2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed.shape"
      ],
      "metadata": {
        "id": "WG9RPq8PdCOO",
        "outputId": "1586f0e3-6d25-42e7-82cf-5c83c9534d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1006, 35475)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "best_clf = XGBClassifier(max_depth=5,n_estimators=250, learning_rate=0.1,\n",
        "                         min_child_weight=5,gamma=0.1, subsample = 0.6, colsample_bytree=0.6,\n",
        "                         objective='multi:softprob', seed=1997, reg_lambda=0.01)\n",
        "best_clf = BaggingClassifier(base_estimator=best_clf, n_estimators=3,random_state=1997,bootstrap=False).fit(X_train_transformed,Y_train)\n"
      ],
      "metadata": {
        "id": "QGYkmMhHkp5D"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X_dev_transformed =  vec.transform(X_dev)\n",
        "\n",
        "y_pred_val = best_clf.predict(X_dev_transformed)\n",
        "y_pred_val_prob = best_clf.predict_proba(X_dev_transformed)\n",
        "\n",
        "print(classification_report(Y_dev, y_pred_val))\n",
        "print(\"Log Loss is \",log_loss(Y_dev,y_pred_val_prob)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NEqfY5LzQmv",
        "outputId": "9c1279dc-47e6-4e2f-fe08-27a6169dcf17"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.96      0.69        67\n",
            "           1       0.94      0.57      0.71        28\n",
            "           2       0.61      0.49      0.54        35\n",
            "           3       0.63      0.77      0.69        69\n",
            "           4       0.75      0.27      0.40        11\n",
            "           5       0.00      0.00      0.00        10\n",
            "           6       0.00      0.00      0.00        14\n",
            "           7       0.00      0.00      0.00         6\n",
            "           8       0.00      0.00      0.00         4\n",
            "           9       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.61       252\n",
            "   macro avg       0.35      0.31      0.30       252\n",
            "weighted avg       0.54      0.61      0.54       252\n",
            "\n",
            "Log Loss is  1.3335695532969987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train in whole dataset ###\n"
      ],
      "metadata": {
        "id": "dNRob5qwAPRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the training matrix. Each row corresponds to a domain name and each column to a word present in at least 10 webpages \n",
        "# and at most 50 webpages of domain names. The value of each entry in a row is equal to the tf-idf weight of that word in the \n",
        "# corresponding domain       \n",
        "vec = TfidfVectorizer(decode_error='ignore', strip_accents='unicode', min_df=10, max_df=50, ngram_range=(1,3), stop_words=stopwords.words('english'))\n",
        "X_train_transformed = vec.fit_transform(train_data_preprocessed)\n",
        "X_test_transformed = vec.fit_transform(test_data_preprocessed)\n"
      ],
      "metadata": {
        "id": "mYrIjhl6_yqR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_transformed = vec.fit_transform(test_data_preprocessed)\n"
      ],
      "metadata": {
        "id": "gbVONeRWCEd6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "best_clf = XGBClassifier(max_depth=5,n_estimators=250, learning_rate=0.1,\n",
        "                         min_child_weight=5,gamma=0.1, subsample = 0.6, colsample_bytree=0.6,\n",
        "                         objective='multi:softprob', seed=1997, reg_lambda=0.01)\n",
        "best_clf = BaggingClassifier(base_estimator=best_clf, n_estimators=3,random_state=1997,bootstrap=False).fit(X_train_transformed,y_train)\n"
      ],
      "metadata": {
        "id": "V05HhbUzAAOn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = best_clf.predict(X_test_transformed)\n",
        "y_pred = best_clf.predict_proba(X_test_transformed)"
      ],
      "metadata": {
        "id": "eZGXUn1ECU0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write predictions to a file\n",
        "with open('sample_submission_xgboost_tfidf.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    lst = list()\n",
        "    for i in range(10):\n",
        "        lst.append('class_'+str(i))\n",
        "    lst.insert(0, \"domain_name\")\n",
        "    writer.writerow(lst)\n",
        "    for i,test_host in enumerate(test_domains):\n",
        "        lst = y_pred[i,:].tolist()\n",
        "        lst.insert(0, test_host)\n",
        "        writer.writerow(lst)"
      ],
      "metadata": {
        "id": "a933_AYxH87s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyqXF3H7B9Mg",
        "outputId": "e0580bc9-9003-48e3-fda6-a7f8311c672c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.78210673e-02, 1.11523941e-01, 1.22909933e-01, ...,\n",
              "        3.96031477e-02, 4.23424095e-02, 7.11465478e-02],\n",
              "       [4.06271011e-01, 4.83447127e-02, 3.05153161e-01, ...,\n",
              "        1.77581888e-02, 1.81083474e-02, 3.10245883e-02],\n",
              "       [6.91385940e-04, 9.91723359e-01, 5.10249811e-04, ...,\n",
              "        3.03476723e-03, 1.20857701e-04, 2.06061683e-04],\n",
              "       ...,\n",
              "       [3.31729203e-01, 8.61254036e-02, 1.04041293e-01, ...,\n",
              "        2.70989370e-02, 3.24273296e-02, 5.55569455e-02],\n",
              "       [9.02644455e-01, 1.19204642e-02, 2.08953638e-02, ...,\n",
              "        3.62119661e-03, 4.98448871e-03, 7.42400484e-03],\n",
              "       [2.11842880e-01, 2.20168546e-01, 1.00283362e-01, ...,\n",
              "        1.83545705e-02, 2.19635740e-02, 3.76296602e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}