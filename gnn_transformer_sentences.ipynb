{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svlataki/DataChallenge/blob/main/gnn_transformer_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "9IGGDWigg4EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "dLYZqyIJ1yOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccea9f9c-0786-478c-eb46-4d72015f2e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/gdrive/My Drive/embedding_features.pkl', 'rb') as fp:\n",
        "    features = np.asarray(pickle.load(fp))"
      ],
      "metadata": {
        "id": "TEbuSVQ41v4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read training data\n",
        "train_domains = list()\n",
        "y_train = list()\n",
        "with open(\"/content/gdrive/My Drive/train.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        l = line.split(',')\n",
        "        train_domains.append(l[0])\n",
        "        y_train.append(int(l[1][:-1]))\n",
        "\n",
        "# Read test data\n",
        "test_domains = list()\n",
        "with open(\"/content/gdrive/My Drive/test.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        l = line.split(',')\n",
        "        test_domains.append(l[0])\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.read_edgelist('/content/gdrive/My Drive/edgelist.txt', delimiter=' ', create_using=nx.DiGraph())\n",
        "node_to_idx = dict()\n",
        "for i, node in enumerate(G.nodes()):\n",
        "    node_to_idx[node] = i\n",
        "\n",
        "print('Number of nodes:', G.number_of_nodes())\n",
        "print('Number of edges:', G.number_of_edges())\n",
        "\n",
        "# Read textual content of webpages of domain names\n",
        "text = dict()\n",
        "with zipfile.ZipFile('/content/gdrive/My Drive/domains.zip', \"r\") as zfile:\n",
        "    for filename in zfile.namelist():\n",
        "        if re.search(r'\\.zip$', filename) is not None:\n",
        "            zfiledata = BytesIO(zfile.read(filename))\n",
        "            with zipfile.ZipFile(zfiledata) as zfile2:\n",
        "                text[filename[:-4]] = ''\n",
        "                for name2 in zfile2.namelist():\n",
        "                    file = zfile2.read(name2)\n",
        "                    text[filename[:-4]] += file.decode('utf16') + ' '\n",
        "\n",
        "\n",
        "idx = [node_to_idx[domain] for domain in text]\n",
        "\n",
        "# To reduce memory \n",
        "text = None"
      ],
      "metadata": {
        "id": "FhC5tc5jYhyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcafe3a-db76-4c4f-98a9-45b46370f0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 65208\n",
            "Number of edges: 1642073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Yields indices to split data into training and test sets\n",
        "idx_train = [node_to_idx[node] for node in train_domains]\n",
        "idx_test = [node_to_idx[node] for node in test_domains]\n",
        "\n",
        "# Split training set into training and validation sets\n",
        "idx_train, idx_val, y_train, y_val = train_test_split(idx_train, y_train, test_size=0.2)"
      ],
      "metadata": {
        "id": "DMIQXZdwotg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_adjacency(A):\n",
        "    \"\"\"Normalizes adjacency matrix\"\"\"\n",
        "    \n",
        "    n = A.shape[0]\n",
        "    A = A + sp.identity(n)\n",
        "    indegs = A.dot(np.ones(n))\n",
        "    inv_indegs = np.power(indegs, -1)\n",
        "    D = sp.diags(inv_indegs)\n",
        "    A_normalized = D.dot(A)\n",
        "\n",
        "    return A_normalized\n",
        "\n",
        "def sparse_to_torch_sparse(M):\n",
        "    \"\"\"Converts a sparse SciPy matrix to a sparse PyTorch tensor\"\"\"\n",
        "    M = M.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((M.row, M.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(M.data)\n",
        "    shape = torch.Size(M.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "metadata": {
        "id": "O0rY-hgvE-TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 130\n",
        "n_hidden = 110 \n",
        "lr = 0.01\n",
        "dropout_rate = 0.6\n",
        "n_class = 10\n",
        "\n",
        "# Read data\n",
        "n = G.number_of_nodes() # Number of nodes\n",
        "adj = nx.adjacency_matrix(G).T\n",
        "adj = normalize_adjacency(adj)\n",
        "\n",
        "# Initialize the features of the nodes as random vectors of dimension 8\n",
        "updated_features = np.zeros((n, 768))\n",
        "for i in range(features.shape[0]):\n",
        "    updated_features[idx[i],:] = features[i,:]"
      ],
      "metadata": {
        "id": "YfKShYk_pHQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def sparse_to_torch_sparse(M):\n",
        "    \"\"\"Converts a sparse SciPy matrix to a sparse PyTorch tensor\"\"\"\n",
        "    M = M.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((M.row, M.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(M.data)\n",
        "    shape = torch.Size(M.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    \"\"\"Simple GNN model\"\"\"\n",
        "    def __init__(self, n_feat, hidden_dim, n_class, dropout):\n",
        "        super(GNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(n_feat, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, n_class)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x_in, adj):\n",
        "        # Implement the graph neural network\n",
        "        # Add 2 message passing layers followed by a fully connected layer\n",
        "\n",
        "        h1 = self.fc1(x_in)\n",
        "        h1 = self.relu(torch.mm(adj, h1))\n",
        "        h1 = self.dropout(h1)\n",
        "\n",
        "        h2 = self.fc2(h1)\n",
        "        h2  = self.relu(torch.mm(adj, h2))\n",
        "        h2 = self.dropout(h2)\n",
        "\n",
        "        x = self.fc3(h2)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "KEVjQfXIpPBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Transform the numpy matrices/vectors to torch tensors\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "y_val = torch.LongTensor(y_val).to(device)\n",
        "adj = sparse_to_torch_sparse(adj).to(device)\n",
        "idx_train = torch.LongTensor(idx_train).to(device)\n",
        "idx_val = torch.LongTensor(idx_val).to(device)\n",
        "idx_test = torch.LongTensor(idx_test).to(device)\n",
        "# Creates the model and specifies the optimizer\n",
        "model = GNN(features.shape[1], n_hidden, n_class, dropout_rate).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "AutVOjRBp8Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_features = torch.FloatTensor(updated_features).to(device)\n"
      ],
      "metadata": {
        "id": "xPWjq4mMp9rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "best_val_loss = 100\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()    \n",
        "    optimizer.zero_grad()\n",
        "    output = model(updated_features, adj)\n",
        "    loss = loss_function(output[idx_train], y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    loss_val = 0\n",
        "    output = model(updated_features, adj)\n",
        "    loss_val = loss_function(output[idx_val], y_val)\n",
        "\n",
        "    # Remember best validation loss and save checkpoint\n",
        "    is_best = loss_val <= best_val_loss\n",
        "    best_val_loss = min(loss_val, best_val_loss)\n",
        "    if is_best:\n",
        "        torch.save({\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer' : optimizer.state_dict(),\n",
        "        }, 'model_best.pth.tar')\n",
        "\n",
        "\n",
        "    if epoch%10==0:\n",
        "        print('Epoch: {:03d}, Train Loss: {:.7f}, Val Loss: {:.7f}'.format(epoch, loss, loss_val))\n"
      ],
      "metadata": {
        "id": "FadWx2L5pS6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0807e465-564c-4928-ced5-47cd4e862c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Train Loss: 1.5720944, Val Loss: 1.5549169\n",
            "Epoch: 020, Train Loss: 1.3437759, Val Loss: 1.3842814\n",
            "Epoch: 030, Train Loss: 1.1976866, Val Loss: 1.2732323\n",
            "Epoch: 040, Train Loss: 1.1400360, Val Loss: 1.2861276\n",
            "Epoch: 050, Train Loss: 1.0956504, Val Loss: 1.2261963\n",
            "Epoch: 060, Train Loss: 1.0530578, Val Loss: 1.1815925\n",
            "Epoch: 070, Train Loss: 0.9595652, Val Loss: 1.1445380\n",
            "Epoch: 080, Train Loss: 0.9876394, Val Loss: 1.1179233\n",
            "Epoch: 090, Train Loss: 0.9580511, Val Loss: 1.1017711\n",
            "Epoch: 100, Train Loss: 0.8838323, Val Loss: 1.0930713\n",
            "Epoch: 110, Train Loss: 0.8625192, Val Loss: 1.0788170\n",
            "Epoch: 120, Train Loss: 0.8241060, Val Loss: 1.0377978\n",
            "Epoch: 130, Train Loss: 0.7965914, Val Loss: 1.0352809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_argmax = torch.exp(output[idx_val]).detach().cpu().numpy().argmax(axis=1)\n",
        "y_pred_prob = torch.exp(output[idx_val]).detach().cpu().numpy()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "print(classification_report(y_val.detach().cpu().numpy(), y_pred_argmax))\n",
        "print(\"Log Loss is \",log_loss(y_val.detach().cpu().numpy(),y_pred_prob)) "
      ],
      "metadata": {
        "id": "zb7b69V2qZxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5efcd77-ee7f-4bed-f9c2-1b39c6ee5f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.84      0.79        62\n",
            "           1       0.76      0.83      0.79        23\n",
            "           2       0.58      0.65      0.61        34\n",
            "           3       0.84      0.79      0.81        77\n",
            "           4       0.54      0.88      0.67         8\n",
            "           5       0.43      0.67      0.52         9\n",
            "           6       0.29      0.24      0.26        17\n",
            "           7       0.00      0.00      0.00        10\n",
            "           8       0.00      0.00      0.00         3\n",
            "           9       0.83      0.56      0.67         9\n",
            "\n",
            "    accuracy                           0.70       252\n",
            "   macro avg       0.50      0.54      0.51       252\n",
            "weighted avg       0.67      0.70      0.68       252\n",
            "\n",
            "Log Loss is  1.03528083605242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.exp(output[idx_test]).detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "qmEBx12uqi_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write predictions to a file\n",
        "with open('sample_submission_bert_sentences.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    lst = list()\n",
        "    for i in range(10):\n",
        "        lst.append('class_'+str(i))\n",
        "    lst.insert(0, \"domain_name\")\n",
        "    writer.writerow(lst)\n",
        "    for i,test_host in enumerate(test_domains):\n",
        "        lst = y_pred[i,:].tolist()\n",
        "        lst.insert(0, test_host)\n",
        "        writer.writerow(lst)"
      ],
      "metadata": {
        "id": "gqw2LRTLqpJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "gnn_transformer_sentences.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCeaJwVW93Ndoob5xaL6/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}